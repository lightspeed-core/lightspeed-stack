# .github/workflows/e2e_test.yml
name: E2E Tests

on: [push, pull_request]

jobs:
  e2e_tests:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - uses: 1arp/create-a-file-action@0.4.5
        env: 
            OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        with:
          path: '.'
          isAbsolutePath: false
          file: 'lightspeed-stack.yaml'
          content: |
            name: lcore
            service:
              host: localhost
              port: 8080
              auth_enabled: false
              workers: 1
              color_log: true
              access_log: true
            llama_stack:
              # use_as_library_client: true
              # library_client_config_path: /app-root/run.yaml
              url: http://lightspeednet:8321
              api_key: "$OPENAI_API_KEY"
            user_data_collection:
              feedback_disabled: false
              feedback_storage: "/tmp/data/feedback"
              transcripts_disabled: false
              transcripts_storage: "/tmp/data/transcripts"
      
      - uses: 1arp/create-a-file-action@0.4.5
        env: 
            OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        with:
          path: '.'
          isAbsolutePath: false
          file: 'run.yaml'
          content: |
            version: '2'
            image_name: simplest-llamastack-app

            apis:
              - agents
              - datasetio
              - eval
              - inference
              - post_training
              - safety
              - scoring
              - telemetry
              - tool_runtime
              - vector_io
            benchmarks: []
            container_image: null
            datasets: []
            external_providers_dir: null
            inference_store:
              db_path: /app-root/.llama/distributions/ollama/inference_store.db
              type: sqlite
            logging: null
            metadata_store:
              db_path: /app-root/.llama/distributions/ollama/registry.db
              namespace: null
              type: sqlite
            providers:
              agents:
              - config:
                  persistence_store:
                    db_path: /app-root/.llama/distributions/ollama/agents_store.db
                    namespace: null
                    type: sqlite
                  responses_store:
                    db_path: /app-root/.llama/distributions/ollama/responses_store.db
                    type: sqlite
                provider_id: meta-reference
                provider_type: inline::meta-reference
              datasetio:
              - config:
                  kvstore:
                    db_path: /app-root/.llama/distributions/ollama/huggingface_datasetio.db
                    namespace: null
                    type: sqlite
                provider_id: huggingface
                provider_type: remote::huggingface
              - config:
                  kvstore:
                    db_path: /app-root/.llama/distributions/ollama/localfs_datasetio.db
                    namespace: null
                    type: sqlite
                provider_id: localfs
                provider_type: inline::localfs
              eval:
              - config:
                  kvstore:
                    db_path: /app-root/.llama/distributions/ollama/meta_reference_eval.db
                    namespace: null
                    type: sqlite
                provider_id: meta-reference
                provider_type: inline::meta-reference
              inference:
                - provider_id: openai
                  provider_type: remote::openai
                  config:
                    api_key: ${{ env.OPENAI_API_KEY }}
              post_training:
              - config:
                  checkpoint_format: huggingface
                  device: cpu
                  distributed_backend: null
                provider_id: huggingface
                provider_type: inline::huggingface
              safety:
              - config:
                  excluded_categories: []
                provider_id: llama-guard
                provider_type: inline::llama-guard
              scoring:
              - config: {}
                provider_id: basic
                provider_type: inline::basic
              - config: {}
                provider_id: llm-as-judge
                provider_type: inline::llm-as-judge
              - config:
                  openai_api_key: '******'
                provider_id: braintrust
                provider_type: inline::braintrust
              telemetry:
              - config:
                  service_name: ''
                  sinks: sqlite
                  sqlite_db_path: /app-root/.llama/distributions/ollama/trace_store.db
                provider_id: meta-reference
                provider_type: inline::meta-reference
              tool_runtime:
                - provider_id: model-context-protocol
                  provider_type: remote::model-context-protocol
                  config: {}
              vector_io:
              - config:
                  kvstore:
                    db_path: /app-root/.llama/distributions/ollama/faiss_store.db
                    namespace: null
                    type: sqlite
                provider_id: faiss
                provider_type: inline::faiss
            scoring_fns: []
            server:
              auth: null
              host: null
              port: 8321
              quota: null
              tls_cafile: null
              tls_certfile: null
              tls_keyfile: null
            shields: []
            vector_dbs: []
            models:
              - model_id: gpt-4-turbo
                provider_id: openai
                model_type: llm
                provider_model_id: gpt-4-turbo

      - name: list files
        run: |
          ls
          cat lightspeed-stack.yaml
          cat run.yaml

      - name: Run service manually
        env: 
            OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          docker compose --version
          docker compose up -d

      - name: Wait for services
        run: |
          echo "Waiting for services to be healthy..."
          sleep 20  # adjust depending on boot time

      - name: Run tests in test container
        run: |
          echo "We got here and the setup is done, yey"
          curl http://localhost:8080/v1/models