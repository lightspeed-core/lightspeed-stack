"""Implementation of MCP-specific test steps."""

import json
import time

import requests
from behave import given, then, when  # pyright: ignore[reportAttributeAccessIssue]
from behave.runner import Context

# Mock MCP server configuration
MOCK_MCP_SERVER_URL = "http://localhost:9000"


@given('I set the MCP-HEADERS header with client token for "{server_name}"')
def set_mcp_headers_with_client_token(context: Context, server_name: str) -> None:
    """Set MCP-HEADERS header with a client-provided token.

    Parameters:
        context (Context): Behave context.
        server_name (str): Name of the MCP server to provide token for.
    """
    if not hasattr(context, "auth_headers"):
        context.auth_headers = {}

    # Set MCP-HEADERS with client token
    mcp_headers = {server_name: {"Authorization": "Bearer my-client-token"}}
    context.auth_headers["MCP-HEADERS"] = json.dumps(mcp_headers)
    print(f"ðŸ”‘ Set MCP-HEADERS for server '{server_name}' with client token")


@given("The MCP mock server request log is cleared")
def clear_mcp_mock_server_log(context: Context) -> None:
    """Clear the MCP mock server request log using the debug/clear endpoint.

    Parameters:
        context (Context): Behave context.
    """
    try:
        response = requests.get(f"{MOCK_MCP_SERVER_URL}/debug/clear", timeout=2)
        if response.status_code == 200:
            result = response.json()
            print(
                f"ðŸ§¹ MCP mock server log cleared (status: {result.get('status', 'unknown')})"
            )
        else:
            print(f"âš ï¸  Warning: Clear endpoint returned status {response.status_code}")
    except requests.RequestException as e:
        print(f"âš ï¸  Warning: Could not clear MCP mock server log: {e}")


@when("I send a query that uses MCP tools")
def send_query_with_mcp_tools(context: Context) -> None:
    """Send a query request that will trigger MCP tool discovery.

    Parameters:
        context (Context): Behave context.
    """
    if not hasattr(context, "auth_headers"):
        context.auth_headers = {}

    base_url = f"http://{context.hostname}:{context.port}"
    url = f"{base_url}/v1/query"

    payload = {
        "query": "What tools are available?",
    }

    # Use longer timeout (60s) if testing error handling
    # llama-stack 0.4.2 can be slow to handle tool errors
    timeout = (
        60
        if hasattr(context, "expect_tool_errors") and context.expect_tool_errors
        else 30
    )

    try:
        context.response = requests.post(
            url, json=payload, headers=context.auth_headers, timeout=timeout
        )
        print(f"ðŸ“¤ Sent query request (status: {context.response.status_code})")
    except requests.RequestException as e:
        print(f"âŒ Query request failed: {e}")
        context.response = None


@when("I wait for MCP server to receive requests")
def wait_for_mcp_requests(context: Context) -> None:
    """Wait a brief moment for MCP server to receive and log requests.

    Parameters:
        context (Context): Behave context.
    """
    # Wait for requests to be processed
    time.sleep(2)
    print("â±ï¸  Waited for MCP server to process requests")


@then("The MCP mock server should have received requests")
def check_mcp_server_received_requests(context: Context) -> None:
    """Verify the MCP mock server received at least one request.

    Parameters:
        context (Context): Behave context.
    """
    mock_server_url = MOCK_MCP_SERVER_URL

    try:
        # Mock server debug endpoint can be slow with many requests - use 15s timeout
        response = requests.get(f"{mock_server_url}/debug/requests", timeout=15)
        assert (
            response.status_code == 200
        ), f"Failed to get debug requests: {response.status_code}"

        requests_log = response.json()
        assert isinstance(
            requests_log, list
        ), f"Expected list, got {type(requests_log)}"
        assert len(requests_log) > 0, "MCP mock server received no requests"

        print(f"âœ… MCP mock server received {len(requests_log)} request(s)")
    except requests.RequestException as e:
        raise AssertionError(f"Could not connect to MCP mock server: {e}") from e


@then("The MCP mock server should have received at least {count:d} requests")
def check_mcp_server_request_count(context: Context, count: int) -> None:
    """Verify the MCP mock server received at least N requests.

    Parameters:
        context (Context): Behave context.
        count (int): Minimum expected request count.
    """
    mock_server_url = MOCK_MCP_SERVER_URL

    try:
        # Mock server debug endpoint can be slow with many requests - use 15s timeout
        response = requests.get(f"{mock_server_url}/debug/requests", timeout=15)
        assert (
            response.status_code == 200
        ), f"Failed to get debug requests: {response.status_code}"

        requests_log = response.json()
        actual_count = len(requests_log)
        assert (
            actual_count >= count
        ), f"Expected at least {count} requests, got {actual_count}"

        print(f"âœ… MCP mock server received {actual_count} request(s) (>= {count})")
    except requests.RequestException as e:
        raise AssertionError(f"Could not connect to MCP mock server: {e}") from e


@then(
    'The MCP mock server should have captured Authorization header "{expected_value}" from file-auth server'
)
def check_file_auth_header(context: Context, expected_value: str) -> None:
    """Verify the MCP mock server captured the expected file-based auth header.

    Parameters:
        context (Context): Behave context.
        expected_value (str): Expected Authorization header value.
    """
    mock_server_url = MOCK_MCP_SERVER_URL

    try:
        # Mock server debug endpoint can be slow with many requests - use 15s timeout
        response = requests.get(f"{mock_server_url}/debug/requests", timeout=15)
        assert response.status_code == 200, "Failed to get debug requests"

        requests_log = response.json()
        # Find requests with the expected auth header
        matching_requests = [
            req
            for req in requests_log
            if req.get("headers", {}).get("Authorization") == expected_value
        ]

        assert (
            len(matching_requests) > 0
        ), f"No requests found with Authorization: {expected_value}"
        print(
            f"âœ… Found {len(matching_requests)} request(s) with file-based auth token"
        )
    except requests.RequestException as e:
        raise AssertionError(f"Could not connect to MCP mock server: {e}") from e


@then(
    'The MCP mock server should have captured Authorization header containing "{token_fragment}" from k8s-auth server'
)
def check_k8s_auth_header(context: Context, token_fragment: str) -> None:
    """Verify the MCP mock server captured k8s token in Authorization header.

    Parameters:
        context (Context): Behave context.
        token_fragment (str): Expected token fragment in Authorization header.
    """
    mock_server_url = MOCK_MCP_SERVER_URL

    try:
        # Mock server debug endpoint can be slow with many requests - use 15s timeout
        response = requests.get(f"{mock_server_url}/debug/requests", timeout=15)
        assert response.status_code == 200, "Failed to get debug requests"

        requests_log = response.json()
        # Find requests with k8s token
        matching_requests = [
            req
            for req in requests_log
            if token_fragment in req.get("headers", {}).get("Authorization", "")
        ]

        assert (
            len(matching_requests) > 0
        ), f"No requests found with k8s token containing: {token_fragment}"
        print(f"âœ… Found {len(matching_requests)} request(s) with k8s auth token")
    except requests.RequestException as e:
        raise AssertionError(f"Could not connect to MCP mock server: {e}") from e


@then(
    'The MCP mock server should have captured Authorization header containing "{token_fragment}" from client-auth server'
)
def check_client_auth_header(context: Context, token_fragment: str) -> None:
    """Verify the MCP mock server captured client token in Authorization header.

    Parameters:
        context (Context): Behave context.
        token_fragment (str): Expected token fragment in Authorization header.
    """
    mock_server_url = MOCK_MCP_SERVER_URL

    try:
        # Mock server debug endpoint can be slow with many requests - use 15s timeout
        response = requests.get(f"{mock_server_url}/debug/requests", timeout=15)
        assert response.status_code == 200, "Failed to get debug requests"

        requests_log = response.json()
        # Find requests with client token
        matching_requests = [
            req
            for req in requests_log
            if token_fragment in req.get("headers", {}).get("Authorization", "")
        ]

        assert (
            len(matching_requests) > 0
        ), f"No requests found with client token containing: {token_fragment}"
        print(
            f"âœ… Found {len(matching_requests)} request(s) with client-provided token"
        )
    except requests.RequestException as e:
        raise AssertionError(f"Could not connect to MCP mock server: {e}") from e


@then('The MCP mock server request log should contain tool "{tool_name}"')
def check_mcp_tool_in_log(context: Context, tool_name: str) -> None:
    """Verify the MCP mock server received requests for a specific tool.

    Queries the mock server's debug endpoint to check the request log.

    Parameters:
        context (Context): Behave context.
        tool_name (str): Expected tool name (e.g., mock_tool_file, mock_tool_k8s).
    """
    mock_server_url = MOCK_MCP_SERVER_URL

    try:
        # Mock server debug endpoint can be slow with many requests - use 15s timeout
        response = requests.get(f"{mock_server_url}/debug/requests", timeout=15)
        assert response.status_code == 200, "Failed to get debug requests"

        requests_log = response.json()

        # Check if any request in the log contains the expected tool name
        found = False
        for req in requests_log:
            if req.get("tool_name") == tool_name:
                found = True
                break

        assert found, f"Tool '{tool_name}' not found in mock server request log"
        print(f"âœ… Tool '{tool_name}' found in MCP server request log")
    except requests.RequestException as e:
        raise AssertionError(f"Could not connect to MCP mock server: {e}") from e


@then('The MCP mock server request log should not contain tool "{tool_name}"')
def check_mcp_tool_not_in_log(context: Context, tool_name: str) -> None:
    """Verify the MCP mock server did NOT receive requests for a specific tool.

    Queries the mock server's debug endpoint to check the request log.
    This is useful for verifying that servers were skipped due to auth issues.

    Parameters:
        context (Context): Behave context.
        tool_name (str): Tool name that should NOT be present.
    """
    mock_server_url = MOCK_MCP_SERVER_URL

    try:
        # Mock server debug endpoint can be slow with many requests - use 15s timeout
        response = requests.get(f"{mock_server_url}/debug/requests", timeout=15)
        assert response.status_code == 200, "Failed to get debug requests"

        requests_log = response.json()

        # Check if any request in the log contains the tool name
        for req in requests_log:
            if req.get("tool_name") == tool_name:
                raise AssertionError(
                    f"Tool '{tool_name}' unexpectedly found in mock server request log "
                    f"(server should have been skipped)"
                )

        print(f"âœ… Tool '{tool_name}' correctly absent from MCP server request log")
    except requests.RequestException as e:
        raise AssertionError(f"Could not connect to MCP mock server: {e}") from e


@then("The MCP mock server request log should contain exactly tools {tool_list}")
def check_mcp_exact_tools_in_log(context: Context, tool_list: str) -> None:
    """Verify MCP server called at least one expected tool and no unexpected tools.

    This validates:
    1. At least ONE tool from the expected list was called (flexible for LLM non-determinism)
    2. NO tools outside the expected list were called (enforces security/auth boundaries)

    This approach balances LLM flexibility with security enforcement - the LLM can choose
    which tools to use, but cannot access tools outside the allowed set.

    Parameters:
        context (Context): Behave context.
        tool_list (str): Comma-separated list of allowed tool names.
    """
    mock_server_url = MOCK_MCP_SERVER_URL

    # Parse expected tools
    expected_tools = [tool.strip() for tool in tool_list.split(",")]

    try:
        # Mock server debug endpoint can be slow with many requests - use 15s timeout
        response = requests.get(f"{mock_server_url}/debug/requests", timeout=15)
        assert response.status_code == 200, "Failed to get debug requests"

        requests_log = response.json()

        # Extract unique tool names from log
        found_tools = set()
        for req in requests_log:
            tool_name = req.get("tool_name")
            if tool_name:
                found_tools.add(tool_name)

        # Check 1: At least ONE expected tool was called
        # (Allows for LLM non-determinism in tool selection)
        called_expected_tools = found_tools & set(expected_tools)
        if not called_expected_tools:
            raise AssertionError(
                f"None of the expected tools were called. "
                f"Expected at least one of: {', '.join(expected_tools)}. "
                f"Found tools: {', '.join(sorted(found_tools))}"
            )

        # Check 2: NO unexpected tools were called
        # (Enforces security - prevents access to unauthorized tools)
        unexpected_tools = [tool for tool in found_tools if tool not in expected_tools]
        if unexpected_tools:
            raise AssertionError(
                f"Unexpected tools found in log: {', '.join(unexpected_tools)}. "
                f"Only expected: {', '.join(expected_tools)}"
            )

        print(
            f"âœ… Tool usage validated: called {', '.join(sorted(called_expected_tools))} "
            f"(allowed: {', '.join(expected_tools)})"
        )
    except requests.RequestException as e:
        raise AssertionError(f"Could not connect to MCP mock server: {e}") from e


@when("I send a query asking about available tools")
def send_query_about_tools(context: Context) -> None:
    """Send a query asking about available tools.

    Parameters:
        context (Context): Behave context.
    """
    if not hasattr(context, "auth_headers"):
        context.auth_headers = {}

    base_url = f"http://{context.hostname}:{context.port}"
    url = f"{base_url}/v1/query"

    model = getattr(context, "default_model", "gpt-4o-mini")
    provider = getattr(context, "default_provider", "openai")

    payload = {
        "query": "What tools are available to help me?",
        "model": model,
        "provider": provider,
    }

    try:
        context.response = requests.post(
            url, json=payload, headers=context.auth_headers, timeout=30
        )
        print(f"ðŸ“¤ Sent query about tools (status: {context.response.status_code})")
    except requests.RequestException as e:
        print(f"âŒ Query request failed: {e}")
        context.response = None


@when("I send a query that explicitly requests tool usage")
def send_query_requesting_tool_usage(context: Context) -> None:
    """Send a query that explicitly asks to use a tool.

    Parameters:
        context (Context): Behave context.
    """
    if not hasattr(context, "auth_headers"):
        context.auth_headers = {}

    base_url = f"http://{context.hostname}:{context.port}"
    url = f"{base_url}/v1/query"

    model = getattr(context, "default_model", "gpt-4o-mini")
    provider = getattr(context, "default_provider", "openai")

    payload = {
        "query": "Please use the mock_tool_k8s tool to test the connection",
        "model": model,
        "provider": provider,
    }

    try:
        context.response = requests.post(
            url, json=payload, headers=context.auth_headers, timeout=30
        )
        print(
            f"ðŸ“¤ Sent query requesting tool usage (status: {context.response.status_code})"
        )
    except requests.RequestException as e:
        print(f"âŒ Query request failed: {e}")
        context.response = None


@when("I send a query that triggers MCP tool usage")
def send_query_triggering_tool_usage(context: Context) -> None:
    """Send a query that should trigger MCP tool usage.

    Parameters:
        context (Context): Behave context.
    """
    if not hasattr(context, "auth_headers"):
        context.auth_headers = {}

    base_url = f"http://{context.hostname}:{context.port}"
    url = f"{base_url}/v1/query"

    model = getattr(context, "default_model", "gpt-4o-mini")
    provider = getattr(context, "default_provider", "openai")

    payload = {
        "query": "Use available tools to help me",
        "model": model,
        "provider": provider,
    }

    try:
        context.response = requests.post(
            url, json=payload, headers=context.auth_headers, timeout=30
        )
        print(
            f"ðŸ“¤ Sent query triggering tools (status: {context.response.status_code})"
        )
    except requests.RequestException as e:
        print(f"âŒ Query request failed: {e}")
        context.response = None


@when("I wait for MCP server to process tool calls")
def wait_for_tool_calls(context: Context) -> None:
    """Wait for MCP server to process tool call requests.

    Parameters:
        context (Context): Behave context.
    """
    time.sleep(3)
    print("â±ï¸  Waited for MCP server to process tool calls")


@when("I send a query that requires multiple tool calls")
def send_query_requiring_multiple_tools(context: Context) -> None:
    """Send a query that should trigger multiple tool calls.

    Parameters:
        context (Context): Behave context.
    """
    if not hasattr(context, "auth_headers"):
        context.auth_headers = {}

    base_url = f"http://{context.hostname}:{context.port}"
    url = f"{base_url}/v1/query"

    model = getattr(context, "default_model", "gpt-4o-mini")
    provider = getattr(context, "default_provider", "openai")

    payload = {
        "query": "Use all available tools to gather information",
        "model": model,
        "provider": provider,
    }

    try:
        # Multiple tool calls can take longer - use 120s timeout
        # Note: This test is timing-sensitive in CI. Locally completes in ~8s,
        # but CI can take 90+ seconds due to container overhead and resource limits
        context.response = requests.post(
            url, json=payload, headers=context.auth_headers, timeout=120
        )
        print(
            f"ðŸ“¤ Sent query requiring multiple tools (status: {context.response.status_code})"
        )
    except requests.RequestException as e:
        print(f"âŒ Query request failed: {e}")
        context.response = None


@then("The MCP mock server should have received tools/list method calls")
def check_tools_list_calls(context: Context) -> None:
    """Verify MCP server responds to tools/list method calls.

    Parameters:
        context (Context): Behave context.
    """
    mock_server_url = MOCK_MCP_SERVER_URL

    # Make an actual tools/list JSON-RPC request
    payload = {"jsonrpc": "2.0", "id": 1, "method": "tools/list", "params": {}}

    try:
        response = requests.post(mock_server_url, json=payload, headers={}, timeout=5)
        assert response.status_code == 200, f"Bad status: {response.status_code}"

        result = response.json()
        assert "result" in result, "Response missing 'result' field"
        assert "tools" in result["result"], "Result missing 'tools' field"
        assert (
            len(result["result"]["tools"]) > 0
        ), "No tools returned in tools/list response"

        print(
            f"âœ… MCP server responded to tools/list with {len(result['result']['tools'])} tool(s)"
        )
    except requests.RequestException as e:
        raise AssertionError(f"Could not connect to MCP mock server: {e}") from e


@then("The MCP mock server should have received tools/call method")
def check_tools_call_method(context: Context) -> None:
    """Verify MCP server received tools/call method.

    Parameters:
        context (Context): Behave context.
    """
    mock_server_url = MOCK_MCP_SERVER_URL

    try:
        # Mock server debug endpoint can be slow with many requests - use 15s timeout
        response = requests.get(f"{mock_server_url}/debug/requests", timeout=15)
        assert response.status_code == 200, "Failed to get debug requests"

        requests_log = response.json()
        assert len(requests_log) > 0, "No requests received by MCP server"
        print("âœ… MCP server received tool execution requests")
    except requests.RequestException as e:
        raise AssertionError(f"Could not connect to MCP mock server: {e}") from e


@then("The response should contain MCP tool execution results")
def check_response_has_tool_results(context: Context) -> None:
    """Verify response contains MCP tool execution results.

    Parameters:
        context (Context): Behave context.
    """
    assert context.response is not None, "No response received"
    assert (
        context.response.status_code == 200
    ), f"Bad status: {context.response.status_code}"

    response_data = context.response.json()
    # Check if response has expected structure
    assert "response" in response_data, "Response missing 'response' field"
    print("âœ… Response contains tool execution results")


@then("The response should indicate successful tool execution")
def check_successful_tool_execution(context: Context) -> None:
    """Verify response indicates successful tool execution.

    Parameters:
        context (Context): Behave context.
    """
    assert context.response is not None, "No response received"
    response_data = context.response.json()

    # For now, just check that we got a valid response
    # In a real scenario, you'd check for tool call metadata
    assert "response" in response_data, "Response missing expected fields"
    print("âœ… Tool execution completed successfully")


@then("The response should contain tool call information")
def check_response_has_tool_info(context: Context) -> None:
    """Verify response contains tool call information.

    Parameters:
        context (Context): Behave context.
    """
    assert context.response is not None, "No response received"
    response_data = context.response.json()

    assert "response" in response_data, "Response missing 'response' field"
    print("âœ… Response contains tool call information")


@then("The tool execution results should be included in the response")
def check_tool_results_in_response(context: Context) -> None:
    """Verify tool execution results are in the response.

    Parameters:
        context (Context): Behave context.
    """
    assert context.response is not None, "No response received"
    response_data = context.response.json()

    # Check response structure
    assert "response" in response_data, "Response missing 'response' field"
    print("âœ… Tool execution results included in response")


@given("The MCP mock server is configured to return errors")
def configure_mock_server_errors(context: Context) -> None:
    """Configure mock server to return errors via MCP-HEADERS.

    Sends the special "Bearer error-mode" token via MCP-HEADERS so all
    configured MCP servers (mock-file-auth, mock-k8s-auth, mock-client-auth)
    receive it and return errors. This token must be propagated through
    MCP-HEADERS, not the top-level Authorization header, because the stack
    only forwards MCP-HEADERS to MCP servers.

    Parameters:
        context (Context): Behave context.
    """
    if not hasattr(context, "auth_headers"):
        context.auth_headers = {}

    # Configure all MCP servers to use error-mode token via MCP-HEADERS
    # The mock server recognizes "Bearer error-mode" and returns errors
    mcp_headers = {
        "mock-file-auth": {"Authorization": "Bearer error-mode"},
        "mock-k8s-auth": {"Authorization": "Bearer error-mode"},
        "mock-client-auth": {"Authorization": "Bearer error-mode"},
    }
    context.auth_headers["MCP-HEADERS"] = json.dumps(mcp_headers)
    context.expect_tool_errors = True
    print(
        "âš ï¸  MCP mock server configured for error mode (error-mode token via MCP-HEADERS)"
    )


@then("The response should indicate tool execution failed")
def check_tool_execution_failed(context: Context) -> None:
    """Verify response indicates tool execution failed.

    Parameters:
        context (Context): Behave context.
    """
    assert context.response is not None, "No response received"
    assert (
        context.response.status_code == 200
    ), f"Bad status: {context.response.status_code}"

    # In error mode, the response should still be 200 but contain error information
    # The LLM will handle the tool error gracefully
    print("âœ… Response received (tool errors are handled gracefully by LLM)")


@then("The MCP mock server should confirm error mode is active")
def check_mock_server_error_mode(context: Context) -> None:
    """Verify the mock server is returning errors via API query.

    Sends a test request to the mock server and confirms it returns isError=true.

    Parameters:
        context (Context): Behave context.
    """
    mock_server_url = MOCK_MCP_SERVER_URL

    try:
        # Verify the mock server is in error mode by checking its response
        payload = {
            "jsonrpc": "2.0",
            "id": 1,
            "method": "tools/call",
            "params": {"name": "test", "arguments": {}},
        }
        response = requests.post(
            mock_server_url,
            json=payload,
            headers={"Authorization": "Bearer error-mode"},
            timeout=5,
        )
        result = response.json()
        assert result.get("result", {}).get(
            "isError"
        ), "Mock server not returning errors"
        print("âœ… Mock server confirmed to be returning errors (isError: true)")
    except requests.RequestException as e:
        raise AssertionError(f"Could not verify mock server error mode: {e}") from e


@then("The MCP mock server should have received multiple tools/call methods")
def check_multiple_tool_calls(context: Context) -> None:
    """Verify MCP server received multiple tool call requests.

    Parameters:
        context (Context): Behave context.
    """
    mock_server_url = MOCK_MCP_SERVER_URL

    try:
        # Mock server debug endpoint can be slow with many requests - use 15s timeout
        response = requests.get(f"{mock_server_url}/debug/requests", timeout=15)
        assert response.status_code == 200, "Failed to get debug requests"

        requests_log = response.json()
        # We expect multiple requests (at least for discovery + calls)
        assert (
            len(requests_log) >= 3
        ), f"Expected multiple requests, got {len(requests_log)}"
        print(f"âœ… MCP server received {len(requests_log)} requests (multiple tools)")
    except requests.RequestException as e:
        raise AssertionError(f"Could not connect to MCP mock server: {e}") from e


@then("All tool calls should have succeeded")
def check_all_tool_calls_succeeded(context: Context) -> None:
    """Verify all tool calls succeeded.

    Parameters:
        context (Context): Behave context.
    """
    assert context.response is not None, "No response received"
    assert context.response.status_code == 200, "Request failed"
    print("âœ… All tool calls completed successfully")


@then("The response should contain results from all tool calls")
def check_response_has_all_results(context: Context) -> None:
    """Verify response contains results from all tool calls.

    Parameters:
        context (Context): Behave context.
    """
    assert context.response is not None, "No response received"
    response_data = context.response.json()

    assert "response" in response_data, "Response missing 'response' field"
    print("âœ… Response contains results from all tool calls")


@when("I send a streaming query that uses MCP tools")
def send_streaming_query_with_mcp_tools(context: Context) -> None:
    """Send a streaming query that should use MCP tools.

    Parameters:
        context (Context): Behave context.
    """
    if not hasattr(context, "auth_headers"):
        context.auth_headers = {}

    base_url = f"http://{context.hostname}:{context.port}"
    url = f"{base_url}/v1/streaming_query"

    model = getattr(context, "default_model", "gpt-4o-mini")
    provider = getattr(context, "default_provider", "openai")

    payload = {
        "query": "Use available tools to help me",
        "model": model,
        "provider": provider,
    }

    try:
        context.response = requests.post(
            url, json=payload, headers=context.auth_headers, timeout=30, stream=True
        )
        print(
            f"ðŸ“¤ Sent streaming query with MCP tools (status: {context.response.status_code})"
        )
    except requests.RequestException as e:
        print(f"âŒ Streaming query request failed: {e}")
        context.response = None


@when("I send a streaming query requiring multiple tools")
def send_streaming_query_requiring_multiple_tools(context: Context) -> None:
    """Send a streaming query requiring multiple tool calls.

    Parameters:
        context (Context): Behave context.
    """
    if not hasattr(context, "auth_headers"):
        context.auth_headers = {}

    base_url = f"http://{context.hostname}:{context.port}"
    url = f"{base_url}/v1/streaming_query"

    model = getattr(context, "default_model", "gpt-4o-mini")
    provider = getattr(context, "default_provider", "openai")

    payload = {
        "query": "Use all available tools to gather comprehensive information",
        "model": model,
        "provider": provider,
    }

    try:
        # Multiple tool calls can take longer - use 120s timeout
        # Note: This test is timing-sensitive in CI. Locally completes in ~8s,
        # but CI can take 90+ seconds due to container overhead and resource limits
        context.response = requests.post(
            url, json=payload, headers=context.auth_headers, timeout=120, stream=True
        )
        print(
            f"ðŸ“¤ Sent streaming query requiring multiple tools (status: {context.response.status_code})"
        )
    except requests.RequestException as e:
        print(f"âŒ Streaming query request failed: {e}")
        context.response = None


@then("The streaming response should be successful")
def check_streaming_response_successful(context: Context) -> None:
    """Verify streaming response was successful.

    Parameters:
        context (Context): Behave context.
    """
    assert context.response is not None, "No response received"
    assert (
        context.response.status_code == 200
    ), f"Bad status: {context.response.status_code}"
    print("âœ… Streaming response completed successfully")

    # Add small delay to allow SQLAlchemy connection pool cleanup
    time.sleep(0.5)


@then("The streaming response should contain tool execution results")
def check_streaming_response_has_tool_results(context: Context) -> None:
    """Verify streaming response contains tool execution results.

    Parameters:
        context (Context): Behave context.
    """
    assert context.response is not None, "No response received"
    assert (
        context.response.status_code == 200
    ), f"Bad status: {context.response.status_code}"

    # For streaming responses, we'd need to parse SSE events
    # For now, just verify we got a successful response
    print("âœ… Streaming response contains tool execution results")

    # Add small delay to allow SQLAlchemy connection pool cleanup
    time.sleep(0.5)
